{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain\n",
    "from hyperdash import Experiment\n",
    "from contextlib import redirect_stdout, redirect_stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence, one_hot\n",
    "from keras.utils import to_categorical, print_summary, plot_model, Sequence\n",
    "from keras.layers import LSTM, CuDNNLSTM, Dense, TimeDistributed, Activation, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_ACTIVE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171161"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r descs\n",
    "len(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Test computation of cumulative KDE.',\n",
       " 'Delete all keys in the current database',\n",
       " 'Arguments to pass to test.',\n",
       " \"Configure the main log according to the process\\\\' configuration and DCNL channel. Sets `mainlog` on self. Returns nothing.\",\n",
       " 'Walk though the jid dir and look for jobs',\n",
       " 'test binding in a date array (with setinputsizes)',\n",
       " 'Tests the _get_module method with no modules.',\n",
       " 'Creates an encoded database value DCNL The result is normally formatted as \"algorithm$salt$hash\" and DCNL must be fewer than 128 characters.',\n",
       " 'Thin wrapper for the Library Import Manager. See ImportManager for details.',\n",
       " 'Creates internal structures for newly registered namespace. DCNL You can register handlers for this namespace afterwards. By default one namespace DCNL already registered (jabber:client or jabber:component:accept depending on context.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(descs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN = max(len(desc) for desc in descs)\n",
    "MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_split(descs):\n",
    "    chars = list(set(chain.from_iterable(desc for desc in descs)))\n",
    "    char_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "    ix_char = {ix:char for ix, char in enumerate(chars)}\n",
    "    descs = [[char_ix[char] for char in desc] for desc in descs]\n",
    "    return descs, char_ix, ix_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.55 s, sys: 68 ms, total: 1.62 s\n",
      "Wall time: 1.63 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('R', 0), ('j', 1), ('b', 2), (';', 3), ('v', 4)],\n",
       " [(0, 'R'), (1, 'j'), (2, 'b'), (3, ';'), (4, 'v')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time descs, dir_map, rev_map = chars_split(descs)\n",
    "list(dir_map.items())[:5], list(rev_map.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(dir_map)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TT prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTSequence(Sequence):\n",
    "    def __init__(self):\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(descs) / BATCH_SIZE))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_ids = self._ids[idx * BATCH_SIZE: (idx + 1) * BATCH_SIZE]\n",
    "        seq_len = max(len(descs[di]) for di in batch_ids)\n",
    "        X = np.zeros((BATCH_SIZE, seq_len, VOCAB_SIZE))\n",
    "        y = np.zeros_like(X)\n",
    "        \n",
    "        for bi, di in enumerate(batch_ids):\n",
    "            for pi, wi in enumerate(descs[di]):\n",
    "                X[bi, pi, wi] = 1\n",
    "            \n",
    "            for pi, wi in enumerate(descs[di][1:]):\n",
    "                y[bi, pi, wi] = 1\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Generate new shuffle in between epochs.\"\"\"\n",
    "        self._ids = np.random.permutation(len(descs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2674, (64, 416, 95), (64, 416, 95))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts = TTSequence()\n",
    "len(tts), tts[0][0].shape, tts[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 100\n",
    "N_LAYERS = 1\n",
    "LSTM_CLASS = GRU  # LSTM if not GPU_ACTIVE else CuDNNLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, None, 100)         58800     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 95)          9595      \n",
      "=================================================================\n",
      "Total params: 68,395\n",
      "Trainable params: 68,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM_CLASS(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), \n",
    "                     dropout=0.3, return_sequences=True))\n",
    "for i in range(N_LAYERS - 1):\n",
    "    model.add(LSTM_CLASS(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDLoss(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.exp = Experiment('2.1.1: ChaRNN convergence', capture_io=False)\n",
    "        \n",
    "        # SUPER-hacky, but it's work\n",
    "        self.exp._hd.out_buf.write = lambda _: _\n",
    "    \n",
    "    def on_train_end(self, logs={}):\n",
    "        self.exp.end()\n",
    "\n",
    "    def on_batch_end(self, n_batch, logs={}):\n",
    "        self.exp.metric('n_batch', n_batch)\n",
    "        self.exp.metric('loss', logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 183/2674 [=>............................] - ETA: 26:51 - loss: 0.7429"
     ]
    }
   ],
   "source": [
    "model.fit_generator(TTSequence(), verbose=1, epochs=1,\n",
    "                    callbacks=[HDLoss()],\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
