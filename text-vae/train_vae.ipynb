{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import nn\n",
    "from corpus import SSTCorpus\n",
    "from tqdm import tqdm_notebook\n",
    "from vanila_model import RnnVae\n",
    "from sklearn.datasets.lfw import Bunch\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Bunch(\n",
    "    model=Bunch(\n",
    "        d_h=64,\n",
    "        d_z=64,\n",
    "        d_c=2,\n",
    "        n_len=15,\n",
    "        n_vocab=10000,\n",
    "        d_emb=50,\n",
    "        p_word_dropout=0.3,\n",
    "        freeze_embeddings=False,\n",
    "    ),\n",
    "    train=Bunch(\n",
    "        n_batch=32,\n",
    "        lr=1e-3,\n",
    "        lr_decay=1000,\n",
    "        n_iter=100000,\n",
    "        log_interval=3000,\n",
    "        grad_clipping=5,\n",
    "        joint_loss=Bunch(\n",
    "            start_inc=3000,\n",
    "            weight=0.01,\n",
    "            w_max=0.15\n",
    "        )\n",
    "    ),\n",
    "    device_code=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    f'cuda:{args.device_code}' \n",
    "    if args.device_code >= 0 and torch.cuda.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = SSTCorpus(**args.model, n_batch=args.train.n_batch, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RnnVae(\n",
       "  (x_emb): Embedding(4847, 50, padding_idx=1)\n",
       "  (encoder_rnn): GRU(50, 64)\n",
       "  (q_mu): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (q_logvar): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (decoder_rnn): GRU(116, 66)\n",
       "  (decoder_fc): Linear(in_features=66, out_features=4847, bias=True)\n",
       "  (encoder): ModuleList(\n",
       "    (0): GRU(50, 64)\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): GRU(116, 66)\n",
       "    (1): Linear(in_features=66, out_features=4847, bias=True)\n",
       "  )\n",
       "  (vae): ModuleList(\n",
       "    (0): Embedding(4847, 50, padding_idx=1)\n",
       "    (1): ModuleList(\n",
       "      (0): GRU(50, 64)\n",
       "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): GRU(116, 66)\n",
       "      (1): Linear(in_features=66, out_features=4847, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RnnVae(**args.model, x_vocab=corpus.vocab('x')).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointLossWithKLAnnealer:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.start_inc = kwargs['start_inc']\n",
    "        self.w_max = kwargs['w_max']\n",
    "        \n",
    "        self.weight = kwargs['weight']\n",
    "        self.inc = (self.w_max - self.weight) / (kwargs['n_iter'] - self.start_inc)\n",
    "    \n",
    "    def __call__(self, i, kl_loss, recon_loss):\n",
    "        if i >= self.start_inc and self.weight < self.w_max:\n",
    "            self.weight += self.inc\n",
    "        \n",
    "        return self.weight * kl_loss + recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c02321eb1f475e8ff8a786ab8abd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more marshall comes is really match looks big and lives .\n",
      "and maid for poetry that was only to be justice .\n",
      "heavy , soggy spirited and funny and meandering .\n",
      "fluffy and obnoxious and .\n",
      "hopkins , rent from robert malkovich .\n",
      "a cockeyed shot all the action characters .\n",
      "terminally brain dead production .\n",
      "a bad mannered , thoroughly long natured .\n",
      "... one of this movie , you can be work as\n",
      "the verdict : two bodies and hardly a laugh up .\n",
      "the film 's hardly a director direction .\n",
      "just another disjointed , fairly predictable and if concept .\n",
      "too good , but a mess .\n",
      "` blue crush ' swims away with better to be a heart .\n",
      "brisk hack job .\n",
      "nevertheless , i take .\n",
      "makes 98 minutes better to trip to feel .\n",
      "... an affecting power , dark and success .\n",
      "the whole thing about the star trek movie in a way time .\n",
      "cherish would 've life for first time when it 's navel\n",
      "it 's fun , you felt and to vulgarity , it thinks it\n",
      "contrived pastiche of caper clichÃ©s and pacing are .\n",
      "brings of this made the film 's never quite released in this\n",
      "a doa dud from belgium .\n",
      "allen 's plenty to offend be wrong in life .\n",
      "one of the greatest films i come along human film .\n",
      "as a pretty pretty much completely .\n",
      "he watches them 's direction has up with passion for detail .\n",
      "a dreary but ultimately lifeless twist .\n",
      "as steamy as last week 's pork dumplings .\n",
      "the hanukkah spirit foster but nothing sane person touch .\n",
      "what 's better than this movie is so light-key and\n",
      "plays like some of the biggest names in all-defining revelation .\n",
      "they are even life on no level .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_params = lambda: model.vae.parameters()\n",
    "trainer = optim.Adam(get_params(), lr=args.train.lr)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(trainer, factor=0.01)\n",
    "# lr_lambda = lambda e: args.train.lr * (0.5 ** (e // args.train.lr_decay))\n",
    "# lr_schelduer = optim.lr_scheduler.LambdaLR(trainer, lr_lambda)\n",
    "joint_loss = JointLossWithKLAnnealer(**args.train.joint_loss, n_iter=args.train.n_iter)\n",
    "\n",
    "batcher = corpus.batcher('unlabeled', 'train', n_iter=args.train.n_iter)\n",
    "t = tqdm_notebook(enumerate(batcher))\n",
    "losses = []\n",
    "log = []\n",
    "epoch = 0\n",
    "for i, x in t:\n",
    "    # Forward\n",
    "    kl_loss, recon_loss = model(x)\n",
    "    loss = joint_loss(i, kl_loss, recon_loss)\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(get_params(), args.train.grad_clipping)\n",
    "    trainer.step()\n",
    "    trainer.zero_grad()\n",
    "    \n",
    "    # Calc metrics and update t\n",
    "    losses.append(loss.item())\n",
    "    cur_loss = np.mean(losses[-args.train.log_interval:])\n",
    "    kl_weight = joint_loss.weight\n",
    "    lr = trainer.param_groups[0]['lr']\n",
    "    t.set_postfix_str(f'loss={cur_loss:.5f} klw={kl_weight:.3f} lr={lr:.7f}')\n",
    "    t.refresh()\n",
    "    \n",
    "    # Log\n",
    "    if (i > 0 and i % args.train.log_interval == 0) or (i == args.train.n_iter - 1):\n",
    "        epoch += 1\n",
    "        lr_scheduler.step(cur_loss, epoch=epoch)\n",
    "        \n",
    "        sent = corpus.reverse(model.sample_sentence(device=device))\n",
    "        print(sent)\n",
    "        \n",
    "        log.append({\n",
    "            'iter': i,\n",
    "            'loss': cur_loss,\n",
    "            'sent': corpus.reverse(model.sample_sentence(device=device)),\n",
    "            'kl_weight': kl_weight,\n",
    "            'lr': lr\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
